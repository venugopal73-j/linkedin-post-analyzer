"""
LinkedIn Post Optimizer - Streamlit Cloud Version
"""

import streamlit as st
from textblob import TextBlob
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from collections import Counter
import re
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import os
import tomli  # For loading TOML environment variables
import signal  # For timeout handling

# Load environment variables from streamlit.toml if it exists
toml_file = "streamlit.toml"
if os.path.exists(toml_file):
    with open(toml_file, "rb") as f:
        config = tomli.load(f)
        env_vars = config.get("env", {})
        for key, value in env_vars.items():
            os.environ[key] = str(value)

# Initialize NLTK resources safely
def initialize_nltk_resources():
    try:
        for resource in ['punkt', 'punkt_tab', 'vader_lexicon', 'stopwords']:
            nltk.download(resource, quiet=True)
    except Exception as e:
        st.error(f"Error initializing NLTK resources: {e}")

initialize_nltk_resources()

# Initialize tools
analyzer = SentimentIntensityAnalyzer()

# Timeout handler for summarization
class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException("Summarization timed out!")

# Helper Functions for LinkedIn Post Optimization
def flesch_kincaid(text):
    words = len(word_tokenize(text))
    sentences = len(sent_tokenize(text))
    if sentences == 0 or words == 0:
        return 0
    avg_words_per_sentence = words / sentences
    syllables = sum([sum(1 for c in word if c.lower() in 'aeiou') for word in word_tokenize(text)])
    score = round(206.835 - 1.015 * avg_words_per_sentence - 84.6 * (syllables / words))
    return max(0, min(100, score))

def analyze_tone_vader(text):
    vs = analyzer.polarity_scores(text)
    return vs['compound']

def spelling_check(text):
    try:
        blob = TextBlob(text)
        corrected = str(blob.correct())
        if corrected != text:
            return 3  # Some issues found
        return 0
    except:
        return 2  # Unknown error

def detect_hashtags_mentions(text):
    hashtags = re.findall(r'#\w+', text)
    mentions = re.findall(r'@\w+', text)
    return len(hashtags), len(mentions)

def detect_call_to_action(text):
    ctas = ['comment', 'let me know', 'thoughts?', 'what do you think', 'share your view', 'discuss']
    return any(cta in text.lower() for cta in ctas)

def detect_emojis(text):
    return len(re.findall(r'[^\w\s,.!?]', text))

def detect_emotional_appeal(text):
    positive_words = ['inspiring', 'amazing', 'excited', 'thrilled', 'proud', 'success']
    count = sum(1 for word in positive_words if word in text.lower())
    return min(10, count)

def generate_hashtags(post, top_n=3):
    words = nltk.word_tokenize(post.lower())
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word.isalpha() and word not in stop_words]
    word_counts = Counter(filtered_words)
    common_words = word_counts.most_common(top_n)
    return ['#' + word for word, _ in common_words]

# Improved deduplication function
def is_similar_sentence(sent1, sent2, threshold=0.5):
    words1 = set(word_tokenize(sent1.lower()))
    words2 = set(word_tokenize(sent2.lower()))
    overlap = len(words1.intersection(words2))
    avg_len = (len(words1) + len(words2)) / 2
    return overlap / avg_len > threshold if avg_len > 0 else False

def strip_cta(text):
    sentences = sent_tokenize(text)
    cta_keywords = ['comment', 'let me know', 'thoughts?', 'what do you think', 'share your view', 'discuss']
    non_cta_sentences = [s for s in sentences if not any(keyword in s.lower() for keyword in cta_keywords)]
    return ' '.join(non_cta_sentences)

def calculate_score(post):
    readability = flesch_kincaid(post)
    tone = analyze_tone_vader(post)
    spelling_issues = spelling_check(post)
    word_count = len(word_tokenize(post))
    cta = detect_call_to_action(post)
    hashtags, mentions = detect_hashtags_mentions(post)
    emojis = detect_emojis(post)
    emotional_appeal = detect_emotional_appeal(post)

    spelling_score = max(0, 10 - spelling_issues)
    length_score = 10 if 80 <= word_count <= 600 else max(0, 10 - abs(word_count - 340) / 60)

    weights = {
        "readability": 8,
        "tone": 10,
        "grammar": 7,
        "length": 3,
        "cta": 20,
        "hashtags": 15,
        "mentions": 5,
        "emotional": 30,
        "emojis": 15
    }

    weighted_score = (
        (readability / 100 * weights["readability"]) +
        ((tone + 1) / 2 * 10 * weights["tone"] / 10) +
        (spelling_score * weights["grammar"] / 10) +
        (length_score * weights["length"] / 10) +
        (10 * weights["cta"] / 10 if cta else 0) +
        (min(3, hashtags) / 3 * 10 * weights["hashtags"] / 10) +
        (min(2, mentions) / 2 * 10 * weights["mentions"] / 10) +
        (emotional_appeal * weights["emotional"] / 10) +
        (min(5, emojis) / 5 * 10 * weights["emojis"] / 10)
    )

    return round(weighted_score), {
        "Readability": readability,
        "Tone & Sentiment": round((tone + 1) / 2 * 10),
        "Grammar & Style": spelling_score,
        "Length & Structure": length_score,
        "Call-to-Action": 10 if cta else 0,
        "Hashtags": min(10, hashtags * 3.3),
        "Mentions": min(5, mentions * 2.5),
        "Emotional Appeal": emotional_appeal,
        "Engagement Hooks": min(5, emojis * 1.0)
    }

def predict_virality(score):
    if score >= 90:
        return "ðŸ”¥ Likely to go viral"
    elif score >= 75:
        return "ðŸ“ˆ High engagement potential"
    elif score >= 60:
        return "ðŸ‘ Moderate engagement"
    else:
        return "ðŸ“‰ Low engagement unless boosted"

# Fallback optimization if summarization fails
def manual_optimize(post, post_without_cta):
    optimized = post_without_cta
    optimized_sentences = sent_tokenize(optimized)
    
    # Add context
    context_additions = [
        "This tool leverages advanced NLP techniques to analyze content effectively.",
        "Did you know? Engaging posts can increase visibility by over 50% on LinkedIn!",
        "For example, adding a personal touch can make your post more relatable and boost engagement.",
        "Iâ€™m absolutely thrilled to share insights that help professionals grow their presence!",
        "As someone passionate about career growth, Iâ€™ve seen how optimized posts can make a difference."
    ]
    for addition in context_additions:
        if not any(is_similar_sentence(addition, s) for s in optimized_sentences):
            optimized += f" {addition}"
            optimized_sentences.append(addition)

    # Add emotional intro
    emotional_intro = "I'm excited to share that"
    if not any(emotional_intro.lower() in s.lower() for s in optimized_sentences):
        optimized = f"{emotional_intro} {optimized} ðŸŒŸðŸŽ‰ðŸš€"

    # Add CTA
    cta_options = [
        "What strategies have you used to improve engagement? Let me know in the comments! ðŸ’¬",
        "Letâ€™s discuss in the comments below! What do you think? ðŸ¤”",
        "Iâ€™d love to hear your viewsâ€”share them in the comments! ðŸ‘‡"
    ]
    if not detect_call_to_action(optimized):
        for cta in cta_options:
            if not any(is_similar_sentence(cta, s) for s in sent_tokenize(optimized)):
                optimized += f"\n{cta}"
                break

    # Add hashtags
    hashtags = ' '.join(generate_hashtags(optimized))
    if not detect_hashtags_mentions(optimized)[0]:
        optimized += f"\n{hashtags}"

    return optimized

# UI
st.set_page_config(page_title="LinkedIn Post Optimizer", layout="centered")
st.title("LinkedIn Post Optimizer")

post = st.text_area(
    "Paste or write your LinkedIn post below...",
    height=300,
    placeholder="Type or paste your LinkedIn post here..."
)

if post.strip():
    total_score, details = calculate_score(post)
    virality = predict_virality(total_score)

    tab1, tab2, tab3 = st.tabs(["Overview", "Metrics", "Suggestions"])

    with tab1:
        st.subheader("Summary")
        if total_score >= 90:
            st.success(f"Original Quality Score: {total_score}/100")
        elif total_score >= 75:
            st.info(f"Original Quality Score: {total_score}/100")
        else:
            st.warning(f"Original Quality Score: {total_score}/100")
        st.markdown(f"### Virality Prediction: {virality}")

    with tab2:
        st.subheader("Parameter Breakdown")
        for key, value in details.items():
            st.progress(int(value), text=f"{key}: {value}/10")

    with tab3:
        st.subheader("Optimization Suggestions")
        suggested_hashtags = generate_hashtags(post)
        st.markdown("#### Suggested Hashtags:")
        st.code(' '.join(suggested_hashtags))
        if not detect_call_to_action(post):
            st.markdown("#### Add a Call-to-Action:")
            st.code("What are your thoughts? Let me know in the comments!")
        
        @st.cache_resource
        def get_summarizer():
            try:
                st.write("Loading summarization model...")  # Debugging
                from transformers import pipeline
                return pipeline("summarization", model="t5-small")
            except Exception as e:
                st.error(f"Failed to load summarization model: {e}")
                return None

        if st.button("Generate AI-Optimized Version"):
            try:
                summarizer = get_summarizer()
                if summarizer is None:
                    st.error("Summarization model not available. Using manual optimization instead.")
                    post_without_cta = strip_cta(post)
                    optimized = manual_optimize(post, post_without_cta)
                else:
                    with st.spinner("Generating optimized version..."):
                        # Calculate input length
                        st.write("Calculating input length...")  # Debugging
                        input_length = len(word_tokenize(post))
                        # Strip CTA from input to avoid duplication
                        st.write("Stripping CTA...")  # Debugging
                        post_without_cta = strip_cta(post)
                        # Adjust prompt based on input length
                        if input_length < 150:
                            prompt = (f"Expand this LinkedIn post by adding context, a brief explanation, a relevant statistic, and a personal touch, while keeping it professional and engaging, and retaining key details: {post_without_cta}")
                            max_length = max(150, int(input_length * 2))  # Reduced
                            min_length = min(100, max_length - 30)  # Reduced
                        else:
                            prompt = (f"Summarize this LinkedIn post while keeping it professional, engaging, and retaining key details and examples: {post_without_cta}")
                            max_length = max(50, int(input_length * 0.8))
                            min_length = min(50, max_length - 10)
                        
                        # Set timeout for summarization (60 seconds)
                        st.write(f"Starting summarization with max_length={max_length}, min_length={min_length}...")  # Debugging
                        signal.signal(signal.SIGALRM, timeout_handler)
                        signal.alarm(60)  # Set timeout to 60 seconds
                        try:
                            optimized = summarizer(prompt, max_length=max_length, min_length=min_length, do_sample=False, num_beams=4)[0]['summary_text']
                            signal.alarm(0)  # Disable alarm
                        except TimeoutException:
                            st.error("Summarization took too long (over 60 seconds). Using manual optimization instead.")
                            optimized = manual_optimize(post, post_without_cta)
                            signal.alarm(0)  # Disable alarm
                        except Exception as e:
                            st.error(f"Error during summarization: {e}. Using manual optimization instead.")
                            optimized = manual_optimize(post, post_without_cta)
                            signal.alarm(0)  # Disable alarm

                        # Post-process to enhance content and score
                        st.write("Post-processing optimized text...")  # Debugging
                        optimized_sentences = sent_tokenize(optimized)
                        # Add additional context for short posts
                        if input_length < 150:
                            context_additions = [
                                "This tool leverages advanced NLP techniques to analyze how well your resume aligns with job requirements.",
                                "Did you know? Over 70% of resumes get filtered out by ATS systems before reaching a recruiter, making tools like this essential for job seekers.",
                                "For example, a user recently jumped their ATS score from 4 to 8 by adding key terms like 'project management' and 'agile methodology'â€”all thanks to the calculatorâ€™s insights.",
                                "Iâ€™m absolutely thrilled and proud to help job seekers navigate the complexities of ATS systems and land their dream roles!",
                                "As someone whoâ€™s passionate about tech and career growth, Iâ€™ve seen firsthand how tailoring your resume can open doors to amazing opportunities."
                            ]
                            for addition in context_additions:
                                if not any(is_similar_sentence(addition, s) for s in optimized_sentences):
                                    optimized += f" {addition}"
                                    optimized_sentences.append(addition)
                        # Restore key sentences with emotional appeal or examples
                        st.write("Restoring emotional and example sentences...")  # Debugging
                        original_sentences = sent_tokenize(post)
                        emotional_sentences = [s for s in original_sentences if any(word in s.lower() for word in ['inspiring', 'amazing', 'excited', 'thrilled', 'proud', 'success'])]
                        example_sentences = [s for s in original_sentences if 'for example' in s.lower() or 'e.g.' in s.lower()]
                        if emotional_sentences:
                            emotional_sentence = emotional_sentences[0]
                            if not any(is_similar_sentence(emotional_sentence, s) for s in optimized_sentences):
                                optimized += f" {emotional_sentence}"
                                optimized_sentences.append(emotional_sentence)
                        if example_sentences:
                            example_sentence = example_sentences[0]
                            if not any(is_similar_sentence(example_sentence, s) for s in optimized_sentences):
                                optimized += f" {example_sentence}"
                                optimized_sentences.append(example_sentence)
                        # Add emotional intro if not already present
                        emotional_intro = "I'm excited to share that"
                        if not any(emotional_intro.lower() in s.lower() for s in optimized_sentences):
                            optimized = f"{emotional_intro} {optimized} ðŸŒŸðŸŽ‰ðŸš€"
                        # Add CTA if not already present
                        cta_options = [
                            "What strategies have you used to beat ATS systems? Let me know in the comments! ðŸ’¬",
                            "Letâ€™s discuss in the comments below! What do you think? ðŸ¤”",
                            "Iâ€™d love to hear your viewsâ€”share them in the comments! ðŸ‘‡"
                        ]
                        if not detect_call_to_action(optimized):
                            for cta in cta_options:
                                if not any(is_similar_sentence(cta, s) for s in sent_tokenize(optimized)):
                                    optimized += f"\n{cta}"
                                    break
                        # Add hashtags if not already present
                        hashtags = ' '.join(generate_hashtags(optimized))
                        if not detect_hashtags_mentions(optimized)[0]:
                            optimized += f"\n{hashtags}"

                # Recalculate score for the optimized version
                st.write("Recalculating score for optimized version...")  # Debugging
                optimized_score, optimized_details = calculate_score(optimized)
                optimized_virality = predict_virality(optimized_score)
                st.markdown("#### Optimized Version:")
                st.markdown(optimized)
                st.markdown(f"**Optimized Quality Score:** {optimized_score}/100")
                st.markdown(f"**Optimized Virality Prediction:** {optimized_virality}")
                st.download_button("Download Optimized Version", data=optimized, file_name="optimized_linkedin_post.txt")

            except Exception as e:
                st.error(f"Unexpected error during optimization: {e}")

else:
    st.info("Please enter your LinkedIn post above to begin the analysis.")